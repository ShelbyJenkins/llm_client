[package]
authors=["Shelby Jenkins"]
description="Rust library for integrating local LLMs (with llama.cpp) and external LLM APIs."
edition="2021"
homepage="https://github.com/shelbyJenkins/llm-client"
name="llm_client"
readme="README.md"
version="0.0.1"

categories=[
    "ai",
    "ggml",
    "llama-cpp",
    "llm",
    "local-llm",
    "machine-learning",
    "openai",
    "rust",
]
keywords=[
    "ai",
    "anthropic",
    "ggml",
    "llama-cpp",
    "llm",
    "local-llm",
    "machine-learning",
    "openai",
    "rust",
]
license="MIT"
repository="https://github.com/shelbyJenkins/llm-client"

[dependencies]
async-convert="1.0.0"
async-openai="0.17.1"
backoff={version="0.4.0", features=["tokio"]}
base64="0.21.0"
bytes="1.5.0"
chrono="0.4.31"
clap="4.4.11"
derive_builder="0.12.0"
dotenv="0.15.0"
futures="0.3.26"
hex-literal="0.4.1"
hf-hub={version="0.3.2", features=["tokio"]}
lazy_static="1.4.0"
rand="0.8.5"
regex="1.10.2"
reqwest={version="0.11.14", features=["json", "multipart", "stream"]}
secrecy={version="0.8.0", features=["serde"]}
serde={version="1.0.152", features=["derive", "rc"]}
serde_json="1.0.93"
serde_yaml="0.9.27"
sha2="0.10"
thiserror="1.0.38"
tiktoken-rs="0.5.7"
tokio={version="1.25.0", features=["fs", "macros"]}
tokio-stream="0.1.11"
tokio-test="0.4.2"
tokio-util={version="0.7.7", features=["codec", "io-util"]}
tracing="0.1.37"
[features]
default=["rustls"]
native-tls=["reqwest/native-tls"]
native-tls-vendored=["reqwest/native-tls-vendored"]
rustls=["reqwest/rustls-tls-native-roots"]

[[bin]]
name="server_runner"
path="src/providers/llama_cpp/bin/server_runner.rs"
reqwest-eventsource="0.4.0"

[[bin]]
name="model_loader_cli"
path="src/providers/llama_cpp/bin/model_loader_cli.rs"
